{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ec809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import heapq\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "445baf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_attributes(d):\n",
    "    with open(d.filename, 'r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        return (next(reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff38a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data(d):\n",
    "    unsorted_values = []   \n",
    "    with open(d.filename, 'r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)#skip labels\n",
    "        for row in reader:\n",
    "            unsorted_values.append(row)\n",
    "            \n",
    "    return unsorted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392ee56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_values(d):\n",
    "    vals = np.transpose(d.raw)\n",
    "    new_vals = []\n",
    "        \n",
    "    for i in range(len(vals)):\n",
    "        inner = []\n",
    "        for j, val in enumerate(vals[i]):\n",
    "            \n",
    "            try:\n",
    "                inner.append(float(val))\n",
    "            except:\n",
    "                inner.append(val)\n",
    "        new_vals.append(inner)\n",
    "    return new_vals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d13741a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorts a 2d array by some column, using merge sort (smallest to largest)\n",
    "def merge_sort_2d(array, index):######ONLY USE FOR INTEGERS OR DOUBLES########NO STRINGS########\n",
    "\n",
    "    if len(array) == 1:\n",
    "        return array\n",
    "    \n",
    "    \n",
    "    split_location = len(array)//2\n",
    "    \n",
    "    half1 = merge_sort_2d(array[:split_location], index)\n",
    "    half2 = merge_sort_2d(array[split_location:], index)\n",
    "    \n",
    "    merged_list = []\n",
    "    \n",
    "    counter1 = 0\n",
    "    counter2 = 0\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        if(counter1 < len(half1) and counter2 < len(half2)):\n",
    "            if(half1[counter1][index] < half2[counter2][index]):\n",
    "                merged_list.append(half1[counter1])\n",
    "                counter1+=1\n",
    "            else:\n",
    "                merged_list.append(half2[counter2])\n",
    "                counter2+=1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    while(counter1 < len(half1)):#one of these will not be true\n",
    "        merged_list.append(half1[counter1])\n",
    "        counter1+=1\n",
    "    while(counter2 < len(half2)):\n",
    "        merged_list.append(half2[counter2])\n",
    "        counter2+=1\n",
    "        \n",
    "        \n",
    "    return merged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7babf6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_list(data):\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    \n",
    "    return [(x - min_val) / (max_val - min_val) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b543dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_classifications(data): #data will be the final column of the dataset\n",
    "    \n",
    "    classifications = []\n",
    "    \n",
    "    classifications.append(data[0])\n",
    "    for i in range(len(data)):\n",
    "        #make a list with each unique class found\n",
    "        match = False\n",
    "        for c in classifications:\n",
    "            if(data[i] == c):\n",
    "                match = True\n",
    "                break\n",
    "        \n",
    "        if(not match):\n",
    "            classifications.append(data[i])\n",
    "                \n",
    "    return classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abe177be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, filename):\n",
    "        self.filename = f\"{filename}.csv\"\n",
    "        self.raw = get_raw_data(self)\n",
    "        self.attributes = find_attributes(self)\n",
    "        self.number_of_attributes = len(self.attributes)\n",
    "        self.attribute_values = attribute_values(self)\n",
    "        self.classifications =  identify_classifications(self.attribute_values[self.number_of_attributes - 1])#classes will be assumed to be the final coulumn in raw data\n",
    "        \n",
    "    def get_attributes():\n",
    "        return self.attributes\n",
    "    \n",
    "    def sort_by_raw(self,index):#return a dataset object sorted by some specific attribute\n",
    "        return merge_sort_2d(self.raw,index)\n",
    "        \n",
    "    def update_attribute_values(self, new_d):#ex: after sorted by some index\n",
    "        self.attribute_values = new_d\n",
    "        \n",
    "    def restore_raw(self):#once the attributes have been updated, restore raw data so that it can happen again \n",
    "        self.raw = get_raw_data(self)\n",
    "    \n",
    "    def pre_normalize_raw(self):#normalize the data and update the raw data\n",
    "        temp = attribute_values(self)#get values transposed and converted to a floats\n",
    "        temp_2 = []\n",
    "        for i, vals in enumerate(temp):\n",
    "            try:\n",
    "                temp_2.append(normalize_list(vals))#normalize\n",
    "            except TypeError:\n",
    "                temp_2.append(vals)#except strings\n",
    "        self.raw = np.transpose(temp_2)#update raw data so that the yoink works more cleanly\n",
    "    \n",
    "    def yoink_data_point(self): #grab a random point and reset the dataset to not include it\n",
    "        p = random.randint(0,149)\n",
    "        self.pre_normalize_raw()\n",
    "        point = self.raw[p]\n",
    "        \n",
    "        self.raw = np.delete(self.raw, p, axis=0)#change raw data\n",
    "        self.attribute_values = attribute_values(self)#recreate attribute values excluding point yoinked\n",
    "        self.restore_raw()\n",
    "        \n",
    "        converted_point = []\n",
    "        \n",
    "        for p in point:\n",
    "            try:\n",
    "                converted_point.append(float(p))\n",
    "            except ValueError:\n",
    "                converted_point.append(p)\n",
    "        return converted_point\n",
    "    \n",
    "    def post_yoink_restore(self):\n",
    "        self.attribute_values = attribute_values(self)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44c4802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScatterPlot:\n",
    "    def __init__(self, x_vals, y_vals, figsize=(8, 6), title=\"KNN model visual\", xlabel=\"X\", ylabel=\"Y\", grid=True):\n",
    "        self.x_vals = [float(x) for x in x_vals]\n",
    "        self.y_vals = [float(y) for y in y_vals]\n",
    "        self.size = figsize\n",
    "        self.title = title\n",
    "        self.xlabel = xlabel\n",
    "        self.ylabel = ylabel\n",
    "        self.grid = grid\n",
    "        self.data_points = [x_vals,y_vals]  \n",
    "    def display(self):\n",
    "        # Create the plot\n",
    "        plt.figure(figsize = (14, 6))\n",
    "        plt.scatter(self.x_vals, self.y_vals)\n",
    "        plt.title(self.title)\n",
    "        plt.xlabel(self.xlabel)\n",
    "        plt.ylabel(self.ylabel)\n",
    "        plt.grid(self.grid)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba0b3a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(d, p):\n",
    "    \n",
    "    distances = [0] * len(d.attribute_values[0])#list to store each distance\n",
    "    p_location = 0 \n",
    "    for att in d.attribute_values:\n",
    "        counter = 0\n",
    "        for val in att:\n",
    "            try:  \n",
    "                distances[counter] += (val - p[p_location])** 2\n",
    "                counter+=1\n",
    "            except TypeError:#no strings\n",
    "                pass\n",
    "        p_location += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    for i, dist in enumerate (distances):\n",
    "        distances[i] = math.sqrt(dist)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d97c2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Object():\n",
    "    def __init__ (self, d):\n",
    "        self.d = d#the dataset object\n",
    "        self.graph = ScatterPlot(d.attribute_values[0],\n",
    "                                 d.attribute_values[1],\n",
    "                                 \"KNN model(2d)\", \n",
    "                                 d.attributes[0], \n",
    "                                 d.attributes[1])\n",
    "                   #the graph will only display the first two attributes in the list\n",
    "                   #can also serve as an example of how high dimensional data doesn't calssify well using this algorithm\n",
    "    def display(self):\n",
    "        self.graph.display()\n",
    "        \n",
    "    \n",
    "    def categorize(self, p):#takes some point p and decides its category based on the KNN algorithm\n",
    "        neighbor_distances = euclidian_distance(self.d, p)\n",
    "        closest_7 = heapq.nsmallest(7,enumerate(neighbor_distances), key=lambda x: x[1])\n",
    "        \n",
    "        indicies = [idx for idx, dist in closest_7]\n",
    "        \n",
    "        #print(f\"The closest neighbors identified have indicies {indicies}\")\n",
    "        \n",
    "        #use the generic classification list to find classification\n",
    "        \n",
    "        classifications = self.d.classifications\n",
    "        \n",
    "        points = [0] * len(classifications)\n",
    "\n",
    "        for i in range(7):\n",
    "            for j in range(len(classifications)):\n",
    "                if(self.d.attribute_values[self.d.number_of_attributes - 1][indicies[i]] == classifications[j]):#which class are the indicies in\n",
    "                    #add one point to point[j] (give the classification a point)\n",
    "                    points[j] += 1\n",
    "                    break#don't need to compare further\n",
    "                \n",
    "        winner = 0    \n",
    "        for k in range(len(points)):\n",
    "            if(points[k] > winner):\n",
    "                winner = k\n",
    "                \n",
    "        #print(f\"The model has classified the point as {self.d.attributes[self.d.number_of_attributes - 1]}:{classifications[winner]}\")\n",
    "        \n",
    "        return classifications[winner]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4a853ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of correct model outcomes: 0.966\n",
      "Ratio of correct model outcomes: 100.0%\n"
     ]
    }
   ],
   "source": [
    "iris_dataset = Dataset(\"iris\")#this returns an object with attributes and their values \n",
    "\n",
    "model = KNN_Object(iris_dataset)\n",
    "#model.display()\n",
    "\n",
    "\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for i in range(1000):\n",
    "    p = iris_dataset.yoink_data_point()\n",
    "    #print(f\"The datapoint randomly pulled from the set had {iris_dataset.attributes[iris_dataset.number_of_attributes - 1]}: {p[4]}\")\n",
    "    result = model.categorize(p)\n",
    "    iris_dataset.post_yoink_restore()\n",
    "    \n",
    "    if(p[iris_dataset.number_of_attributes - 1] == result):\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "    \n",
    "print(f\"Ratio of correct model outcomes: {correct/(incorrect + correct)}\")\n",
    "\n",
    "diabetes_dataset = Dataset(\"diabetes\")\n",
    "diabetes_model = KNN_Object(diabetes_dataset)\n",
    "\n",
    "\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for i in range(1000):\n",
    "    p = diabetes_dataset.yoink_data_point()\n",
    "    #print(f\"The datapoint randomly pulled from the set had {diabetes_dataset.attributes[diabetes_dataset.number_of_attributes - 1]}: {p[diabetes_dataset.number_of_attributes - 1]}\")\n",
    "    result = diabetes_model.categorize(p)\n",
    "    diabetes_dataset.post_yoink_restore()\n",
    "    \n",
    "    \n",
    "    if(p[diabetes_dataset.number_of_attributes - 1] == result):\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "        \n",
    "    \n",
    "    \n",
    "print(f\"Ratio of correct model outcomes: {(correct/(incorrect + correct)) * 100}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a3542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a80fee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
